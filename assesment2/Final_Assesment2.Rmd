---
title: "Second Assignment"
author: "Malgorzata, Kevin, Johannes"
date: "09th of February, 2019"
output: 
  pdf_document:
    latex_engine: xelatex
fontsize: 11pt
---

## Microeconometrics 2018/2019 
# Assesment 2

### Group Members: 
Johannes Wagner, ID: 598797, Msc Statistics, <wagnejoh@hu-berlin.de>


Malgorzata Paulina Olesiewicz, ID:598939, Msc Statistics, <malgorzata.paulina.olesiewicz@student.hu-berlin.de>


Kevin Hope, ID: 598247, Msc Statistics, <Kevin.Hoppe1@web.de>   

## Approximate individual contributions: 

```{r, echo = FALSE}
suppressWarnings(library("kableExtra"))
e=c("1",
    "2",
    "3",
    "4") 
   
f=c("33 % theory + 100% programming",
    "33% theory",
    "33% theory",
    "33% theory") 
g= c("33 % theory", 
     "33% theory",
     "33% theory + 100% programming",
     "33% theory") 
 
h= c("33% theory", 
     "33% theory",
     "33% theory",
      "33% theory+100% programming")
work = data.frame(e,f,g,h)
colnames(work)= c("Task","Johannes","Malgorzata","Kevin")
kable(work)%>% kable_styling() 
```


$$
\pagebreak
$$


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
### Setting up the enviroment
library(foreign, quietly = TRUE)
library(tidyverse, quietly = TRUE)
library(margins, quietly = TRUE)
library(mfx, quietly = TRUE)
library(caret,quietly = TRUE)
# install.packages("InformationValue")
library(InformationValue)
library(xtable)
library(olsrr)
#install.packages("nnet")
library(nnet)
library(dplyr)
#install.packages("mlogit")
library(mlogit)
```

```{r, include = FALSE}
# clear environment
rm(list = ls())
# Define Directory
dir <- "~/R/git/Micro_Ass1"
setwd(dir)
### loading the data and looking up meaning of the variables
data <- read.dta("loanapp1.dta")
(variables=cbind(colnames(data),attr(data, "var.labels")))
# Model Selection ---------------------------------------------------------
# For simplicity reasons we can use a simple OLS model for the selection process
# remove variables which are directly related to rejection and have no explanatory power given our social theory: unver
# remove missings for the purpose of model selection
data_woNA <- data[complete.cases(data),] %>% dplyr::select(-c(approve, action, unver))
# names(data_woNA)
# regress the depend variable on all regressors
fit <- lm(reject~.,data=data_woNA)
## Perform forward stepwise selection to find most important variables
#ols_step_forward_p(fit, details = FALSE, penter = 0.001)
## Select the four significant variables which can be explained trough our theory:
# inson, pubrec, white, obrat
```

```{r, echo=FALSE}
## Calculate logit model with MPEs at means
dataSelect <- dplyr::select(data, c(reject, sch, married, netw, inson, pubrec, white, obrat))
dataSelect <- dataSelect[complete.cases(dataSelect),] 
# logit_results <- logitmfx(reject~sch + married + inson + cons + pubrec + white + obrat, data = dataSelect, atmean=TRUE)
# logit_results
model=glm(reject~sch + married + netw + inson + pubrec + white + obrat, data =data, family = "binomial"(link="logit"))
```

## Task 1 : Chosing the variables for the model

The choice of variables should be grounded on both theory and empirical evidence. Our theoretical perspective is guided by the assumption that higher social, cultural and economical capital leads to a lower probability of receiving a loan rejection. Therefore we choose three variables, which represent relevant social characteristics given our theory: marriage, networth and educational level.  
Our next step was to look at a simple linear regression model for all variables of the data set and run a forward stepwise selection process on them. We obtained nine highly relevant variables and choose another four variables from this list, with respect to their suitability to our theory.  

To create the logit model we have chosen followig variables with regard to our theory: 


```{r , echo=FALSE}
a=c("Education (cultural capital)",
    "Marital Status (social capital)",
    "Wealth in Dollar (economic capital)",
    "Appr. private insurance (economic capital)", 
    "Experience of bunkruptcy (economic capital)",
    "Ethnical majority (social & cultural capital)",
    "Ration of obligations vs. income (ecomoic capital)") 
   
b=c("sch",
    "married",
    "netw",
    "inson",
    "pubrec",
    "white",
    "obrat") 
c= c("[0,1]", 
     "[0,1]",
     "[-7919, 28023]",
     "[0,1]",
     "[0,1]",
     "[0,1]",
     "[0, 95]") 
 
d= c("-", 
     "-",
     "-",
     "-",
     "+",
     "-",
     "+") 
vars = data.frame(a,b,c,d)
colnames(vars)= c("Indicator","Variable","Scale", "Expected Effect")
kable(vars)%>% kable_styling()
```



###Results interpretation

You can not directly compare the magnitude of the coefficients of explanatory variables with different scales. For interpretation, we usually want to use coefficients as marginal effects given a little change in the explanatory variable. Since the effect of a "unit change" can mean quite different effects given the scale of the variable, you can not compare variables with different scales. Also one has to keep in mind that the coefficients of the Logit are non-linear parameters. However, in case the variables have the same scale or are standardized, it is possible to compare the size of their coefficients since the logit function is strictly monotonically increasing. Before interpreting the magnitude, one should be aware that the coefficients are random representations of the true parameter and the confidence level of two or more parameters being different needs to be tested. Also coefficients need to be transformed into probabilities before they can be compared and interpreted in a meaningfull way.    

```{r , echo=FALSE}
fm1.table <- xtable(model)
kable(fm1.table, caption = "Logit Model")
```

###Sensitivity and Specificity

The notion of sensitivity and specificity is used to describe how accurately model predicts the binary outcomes. Sensitivity describes the fraction of correctly predicted positive $(Y=1)$ outcomes and specificity describes the fraction of correctly predicted negative $(Y=0)$ outcomes. Our aim is to find an optimal equilibrium between the two fractions, which would allow us simultaneously the best possible prediction of both outcomes.    

In our prediction, we use $"c"$ as a threshold above which the outcome should be predicted as a positive. Consequently, any result of our predicted logit model which will be equal or below the threshold will be assigned as a negative outcome. 

The most logical starting point for the binary response prediction model in $c =0.5 $

```{r, echo=False}

predicted=predict(model,dataSelect, type="response")
sen_1=sensitivity(dataSelect$reject,predicted,threshold = 0.5)
spec_1=specificity(dataSelect$reject,predicted,threshold = 0.5)
vars_1 = data.frame("0.5",sen_1, spec_1)
colnames(vars_1)= c("Threshold","Sensitivity","Specificity")
kable(vars_1)%>% kable_styling()
```


With threshold $c=0.5$ we can observe that almost all (99%) negative responds but only 24%of the positive responds have been predicted correctly.Since the closer the threshold $"c"$ to 1 the higher specificity (the likelihood that we will predict negative respond increases) we will decrease the threshold to 0.3. 


```{r, echo=False}
sen_2=sensitivity(dataSelect$reject,predicted,threshold = 0.3)
spec_2=specificity(dataSelect$reject,predicted,threshold = 0.3)
vars_2 = data.frame("0.3",sen_2, spec_2)
colnames(vars_2)= c("Threshold","Sensitivity","Specificity")
kable(vars_2)%>% kable_styling()
```
We can observe some improvment in prediction of positive outcomes to 36% while keeping the prediction of negative outomes still very accurate - 96%. We are going in the right direction.To find out optimal cut off threshold for the prediction we have used the "InformationValue" package.

```{r, echo=False}
cut= optimalCutoff(actuals = dataSelect$reject, predictedScores =predicted,optimiseFor="Both")
sen_3=sensitivity(dataSelect$reject,predicted,threshold =cut)
spec_3=specificity(dataSelect$reject,predicted,threshold =cut)
vars_3 = data.frame(cut,sen_3, spec_3 )
colnames(vars_3)= c("Optimal Cut off","Sensitivity","Specificity")
kable(vars_3)%>% kable_styling()
```

The optimal cut off level is $c=0.118$ where 65% of positive respons is being predicted correctly and 83% of negative respons is being predicted correctly. 

### Multinomial Logit



```{r, include=FALSE}
## Load data and extract the 7 variables that we chose 
## as a group:
##
## 1. Education (sch): [0,1]
## 2. marital status (married): [0,1]
## 3. Existence of private mortgage insurance (inson): [0,1]
## 4. Net worth (netw): [-7919, 28023]
## 5. Former bankruptcy (pubrec): [0,1]
## 6. Ethnic majority (white): [0,1]
## 7. Ratio of income and liabilities (obrat) : [0-95]
dat.red <- dplyr::select(data, reject, sch, married, inson, cons, pubrec, white, obrat, netw); as.tibble(dat.red)
## Coding categorial variables as dummy variables:
dummy.var <- function(var){
  
  if(min(unique(var), na.rm = T) == 0){
    var <- var + 1
  }
  
  len.var <- length(var)
  levels.var <- var %>% as.factor %>% levels %>% length
  levels.dummy <- levels.var - 1
  
  dummy <- matrix(NA,
                  nrow = len.var,
                  ncol = levels.dummy)
  for(j in 1:levels.var){
    
    match.loc <- which(var == unique(var)[j])
    
    for(i in match.loc){
      
      dummy.vec <- rep(0, levels.dummy)
      dummy.vec[unique(var)[j] - 1] <- 1
      dummy[i,] <- dummy.vec
    }
  }
  return(dummy)
}
sch <- dummy.var(dat.red$sch); head(sch)
married <- dummy.var(dat.red$married); head(married)
inson <- dummy.var(dat.red$inson); head(inson)
#cons <- dummy.var(dat.red$cons); head(cons)
pubrec <- dummy.var(dat.red$pubrec); head(pubrec)
white <- dummy.var(dat.red$white); head(white)
## Put together data set for regression models:
dat <- data.frame("reject" = dat.red$reject,
                  "educ" = sch,
                  "marry" = married,
                  "insur" = inson,
                  "netw" = dat.red$netw,
                  "bankr" = pubrec,
                  "white" = white,
                  "oblig" = dat.red$obrat)
dat.mnl <- mlogit.data(dat, shape = "wide", choice = "reject")
## Formulas for regression models:
form <- 'reject ~ educ + marry + insur + netw + bankr + white + oblig'
form.mnl <- mFormula(appr ~ educ + marry + insur + netw + bankr + white + oblig)
## Estimate a binomial logit model:
head(dat); binom <- glm(formula = form, data = dat, family = "binomial"(link = "logit"))
summary(binom)
## Estimate a multinomial logit model:
head(dat.mnl); multinom <- multinom(formula = form, data = dat)
summary(multinom)
## Comparison of coefficient estimates:
beta.diff <- cbind(coef(binom) - coef(multinom)); beta.diff
```


The difference in the coefficient estimates is zero up to at least the fourth 
decimal place:

```{r, echo = FALSE, include = TRUE}
kable(beta.diff, digit=6 ) %>% kable_styling(position = "center") 
```


This is because the multinomial logit model (MNL) reduces to the 
binomial logit model in case of a binomial dependent variable as can be seen in the formulas. In the MNL, the probability $\pi_{ij}$ of individual $i$ choosing alternative $j$ is given by:

  $$ \pi_{ij_{multinomial}} = \frac{\exp(x_{i}'\beta_{j})}{\sum_{r=1}^J \exp{x_{i}'\beta_{j}}}. $$

Compare this to the binomial logit model, where the probability $\pi_{i}$ of individual $i$ 
picking alternative $j = 1$ is given by:

  $$ \pi_{i_{binomial}} = \frac{\exp(x_{i}'\beta)}{1 + \exp(x_{i}'\beta)}.  $$
  
In the MNL, due to identification constraints, $\beta_{1}$ is fixed at $0$. This establishes $j = 1$ as the reference category. For the remaining $J-1$ categories, $\beta_{j}$ coefficients are estimated. If the dependent variable has only two categories, i.e. $J = 2$, this means that only one $\beta$ and one $\pi_{i}$ need to be calculated (for the one category that is not the reference category) so the index $j$ in $\pi_{ij}$ and $\beta_{j}$ can be dropped. Since the constraint for $\beta_{1} = 0$ means that $\exp{(x_{i}'\beta_{1})}$ evaluates to $1$, this reduces the denominator in the MNL to $1 + \exp(x_{i}'\beta_{2})$. After dropping the now obsolete index of the coefficient vector, the two formulas given above are equal.

    © 2019 GitHub, Inc.
    Terms
    Privacy
    Security
    Status
    Help

    Contact GitHub
    Pricing
    API
    Training
    Blog
    About