---
  title: "First Assignment"
author: "Malgorzata, Kevin, Johannes"
date: "December, 2018"
output: 
  pdf_document:
  latex_engine: xelatex
fontsize: 11pt
---
  
  ## First Problem: (Simulation: Latent Variable, Probit Model):
  $$
  \begin{array} { l } 
{ \text { Consider the following JOINT distributions of random variables } X _ { 0 }  \&  \varepsilon _ { j } ^ { * } \text { (You must simulate } } \\ { \text { it as a joint (multivariate) normal distributions!). } } 
\end{array}
$$
  $$
  \left( 
    \begin{array} { c } 
    { X _ { 0 } } \\ { \varepsilon _ { 1 } ^ { * } } \end{array} \right) \sim N \left[ \left( \begin{array} { c } { 10 } \\ { 0 } \end{array} \right) , \left( \begin{array} { c c } { 2 ^ { 2 } } & { 0 } \\ { 0 } & { 1 } \end{array} \right) \right] , \left( \begin{array} { c } { X _ { 0 } } \\ { \varepsilon _ { 2 } ^ { * } } \end{array} \right) \sim N \left[ \left( \begin{array} { c } { 10 } \\ { 0 } \end{array} \right) , \left( \begin{array} { c c } { 2 ^ { 2 } } & { 0 } \\ { 0 } & { 2 ^ { 2 } } \end{array} \right) \right] , 
$$
  
  $$
  \left( 
    \begin{array} { c } { X _ { 0 } } \\ { \varepsilon _ { 3 } ^ { * } } \end{array} \right) \sim N \left[ \left( \begin{array} { c } { 10 } \\ { 0 } \end{array} \right) , \left( \begin{array} { c c } { 2 ^ { 2 } } & { 3 } \\ { 3 } & { 2 ^ { 2 } } \end{array} 
                                                                                                                                                                                  \right) \right]
$$
  
  with 

$$
  \begin{array} { l } 
{ \beta _ { 0 } = - 30 } \\ { \beta _ { 1 } = 4 } \\
\\
{ Y _ { j } ^ { * } = \beta _ { 0 } + \beta _ { 1 } * X _ { 1 } + \varepsilon _ { j }} \\
\\
Y _ { j } = \left\{ \begin{array} { l l } { 1 \text { if } Y _ { j } ^ { * } > 0 } \\ { 0 , \text { otherwise } } \end{array} \right.\\
  \\
  j \in \{ 1,2,3 \}
  \end{array}
  $$
    
    > a) [1P] Please simulate the three datasets (i.e.: $Y _ { j } , X _ { 1 } , 
                                                  \varepsilon _ { j } ^ { * } ; j \in \{ 1,2,3 \}$ ) with a sample size of 30000 observations.
For each of the three datasets, estimate a probit model of $\mathrm { Y } _ { \text { i } } \text { on } x _ { 1 }$ 
  and save the estimate for $\hat { \beta } _ { 1 }$.

```{r include=FALSE}
library(tidyverse, quietly = TRUE) # for pipe operator %>%
library(mvtnorm, quietly = TRUE) # for joint multivariate Distribution
library(mfx, quietly = TRUE) # for estimating AMPE

#setwd("~/Desktop/Master_Statistik/WS-18-19/Courses/Microeconometrics/Tutorial/1assigment")
#setwd("C:/Users/Nutzer/Desktop")
getwd()
```


```{r, echo=FALSE}
# Assigment 1 ------------------------------------------------------------------

# clear environment

rm(list = ls())

# set seed for comparibility

set.seed(101)

# ------------------------------------------------------------------------------
## set parameters for the latent variable

beta0 <- -30
beta1 <- 4

# Number of Observations 30.000 (use 400 for fast results)

obs = 30000

# Simulate given Distributions -------------------------------------------------

# Use package mvnorm to simulate multivariate data for a given mean vector 
# and covariance matrix V
# method used described by Ripley (1987, p.98)

## Define covariance matrices v_j for population j = 1,2,3

V1 <- matrix(c(4,0,0,1), nrow=2, ncol=2)

V2 <- matrix(c(4,0,0,4), nrow=2, ncol=2)

V3 <- matrix(c(4,3,3,4), nrow=2, ncol=2)

# 1.a) Simulate three Datasets with Y, xNull and error -------------------

## Simulate distributions with given means and Covariance; round xNull 

distr1 <- rmvnorm(obs, mean = c(10,0), sigma = V1, method = 'eigen') %>% as.data.frame() 
names(distr1) = c('xNull','error1')
distr1$xNull <- round(distr1$xNull,2)

distr2 <- rmvnorm(obs, mean = c(10,0) ,sigma = V2 , method = 'eigen') %>% as.data.frame()
names(distr2) = c('xNull','error2')
distr2$xNull <- round(distr2$xNull,2)

distr3 <- rmvnorm(obs, mean = c(10,0) ,sigma = V3 , method = 'eigen') %>% as.data.frame()
names(distr3) = c('xNull','error3')
distr3$xNull <- round(distr3$xNull,2)

# Set latent variables:

yLatent1 <- beta0 + beta1 * distr1[,"xNull"] + distr1[,"error1"]
yLatent2 <- beta0 + beta1 * distr2[,"xNull"] + distr2[,"error2"]
yLatent3 <- beta0 + beta1 * distr3[,"xNull"] + distr3[,"error3"]

## Dichotomize latent variable:

y1 <- yLatent1 %>% replace(yLatent1<=0,0) %>% replace(yLatent1>0,1)
y2 <- yLatent2 %>% replace(yLatent2<=0,0) %>% replace(yLatent2>0,1)
y3 <- yLatent3 %>% replace(yLatent3<=0,0) %>% replace(yLatent3>0,1)

distr1 <- distr1 %>% mutate(Y = y1)
distr2 <- distr2 %>% mutate(Y = y2)
distr3 <- distr3 %>% mutate(Y = y3)
```

```{r include=FALSE}
# 1.a) Save the estimate for Betaj, j = 1,2,3 -----------------------------

# Probit models
probit1 <- glm(distr1[,"Y"] ~ distr1[,"xNull"], family=binomial(link="probit")) 
probit2 <- glm(distr2[,"Y"] ~ distr2[,"xNull"], family=binomial(link="probit")) 
probit3 <- glm(distr3[,"Y"] ~ distr3[,"xNull"], family=binomial(link="probit")) 
```

```{r, echo=FALSE}
estBeta1 <- round(probit1$coefficients[2], 2)
estBeta2 <- round(probit2$coefficients[2], 2)
estBeta3 <- round(probit3$coefficients[2], 2)

```

For each of the three populations, using our simulated data along with the regression weights given in the assignment the probabilities of the observed values of $y_i|x_i$ are given by:
  $$
  f \left( y _ { i } | \mathbf { x } _ { 1 } \right) = \left[ \Phi \left( \mathbf { x } _ { 1 } ^ { \prime } \mathbf { \beta _ {1} } \right) \right] ^ { y _ { i } } \left[ 1 - \Phi \left( \mathbf { x } _ { 1 } ^ { \prime } \boldsymbol { \beta } _ {1} \right) \right] ^ { 1 - y _ { i } } \quad y _ { i } = 0,1
$$
  Assuming independence of the samples, the parameter estimate in the respective Probit model  $\hat { \beta } _ {1,j  }$ for j = 1,2,3 can be obtained through the maximization of the log likelihood of the probability function $f \left( y _ { i } | \mathbf { x } _ { 1 } \right)$.

The estimates for $\hat { \beta } _ { 1,1 } , \hat { \beta } _ { 1,2 } \text{ and } \hat { \beta } _ { 1,3 }$ are `r estBeta1`, `r estBeta2` and `r estBeta3` respectively.

```{r, echo=FALSE}
## KH:
## - ich bekomme das Markdown Zeugs nicht zum Laufen, was das Ganze etwas
##   unübersichtlich macht
## - daher lies bitte nochmal über den Text drüber, wenn es bei dir zur 
##   pdf kompiliert. kann sein, dass da die Formatierung dann teils nicht 
##   ganz stimmt, wo ich etwas am Text geändert habe
## - habe den text bisschen angepasst, weil ich mich z.b. über die Formulierung
##   "we calculate the probit model" bisschen gewundert hatte. Die Formel in 
##   diesem Block für f(y given x) gibt ja die "Wahrscheinlichkeit für 
##   Beobachtung i" an und wird dann für den MLE verwendet, darum habe ich es 
##   auch so geschrieben, damit wir uns da genau ausdrücken. Brauchen wir aber
##   die Formel hier? Ich würde es an sich lieber simpel halten und vermute auch, 
##   dass Thomas Repasky das auch gut findet
```

> b) [1.5P] Repeat a 400 times while saving all the different estimate in vectors for all
three models (based on the three different populations). Plot the kernel density
estimates for $\hat { \beta } _ { 1 }$ based on the three populations next to each other. Describe shortly
the choices you faced and made estimating the density functions.


```{r, echo=FALSE}
# b.) Repeat estimation 400 times -----------------------------------------
## Number of iterations n

n <- 400

# empty vectors for estimated betas for each population

betaEst1 <- NULL
betaEst2 <- NULL
betaEst3 <- NULL
```

```{r include=FALSE}

for (i in 1:n){
  ## Population 1
  distr1 <- rmvnorm(obs, mean = c(10,0) ,sigma = V1 , method = 'eigen') %>% as.data.frame()
  names(distr1) = c('xOne','error1')
  distr1$xOne <- round(distr1$xOne,2)
  yLatent1 <- beta0 + beta1 * distr1[,"xOne"] + distr1[,"error1"]
  y1 <- yLatent1 %>% replace(yLatent1<=0,0) %>% replace(yLatent1>0,1)
  distr1 <- distr1 %>% mutate(Y = y1)
  probit1 <- suppressWarnings(glm(distr1[,"Y"] ~ distr1[,"xOne"], family=binomial(link="probit")))
  betaEst1[i] <- probit1$coefficients[2]
  ## Population 2
  distr2 <- rmvnorm(obs, mean = c(10,0) ,sigma = V2 , method = 'eigen') %>% as.data.frame()
  names(distr2) = c('xOne','error2')
  distr2$xOne <- round(distr2$xOne,2)
  yLatent2 <- beta0 + beta1 * distr2[,"xOne"] + distr2[,"error2"]
  y2 <- yLatent2 %>% replace(yLatent2<=0,0) %>% replace(yLatent2>0,1)
  distr2 <- distr2 %>% mutate(Y = y2)
  probit2 <- suppressWarnings(glm(distr2[,"Y"] ~ distr2[,"xOne"], family=binomial(link="probit")))
  betaEst2[i] <- probit2$coefficients[2]
  ## Population 3
  distr3 <- rmvnorm(obs, mean = c(10,0) ,sigma = V3 , method = 'eigen') %>% as.data.frame()
  names(distr3) = c('xOne','error3')
  distr3$xOne <- round(distr3$xOne,2)
  yLatent3 <- beta0 + beta1 * distr3[,"xOne"] + distr3[,"error3"]
  y3 <- yLatent3 %>% replace(yLatent3<=0,0) %>% replace(yLatent3>0,1)
  distr3 <- distr3 %>% mutate(Y = y3)
  probit3 <- suppressWarnings(glm(distr3[,"Y"] ~ distr3[,"xOne"], family=binomial(link="probit")))
  betaEst3[i] <- probit3$coefficients[2]
  if(i %in% c(1) | i%%10 == 0){print(paste(i, "von", n))}
}

## Make plots for the answers
# Plot the kernel density estimates for beta1 based on the three populations ----

denBeta1 <- density(betaEst1, bw = "nrd0", adjust = 1, kernel = "gaussian")
denBeta2 <- density(betaEst2, bw = "nrd0", adjust = 1, kernel = "gaussian")
denBeta3 <- density(betaEst3, bw = "nrd0", adjust = 1, kernel = "gaussian")

png(filename="Plot2.png", height = 370)

plot(denBeta1$x, denBeta1$y, type="l", xlim = c(min(betaEst1), max(betaEst1)),
     xlab = "Estimated Coefficient", ylab = "Density", main ="Population 1")
abline(v=mean(betaEst1))
dev.off()
# 
# 
png(filename="Plot1.png")

plot(denBeta1$x,denBeta1$y,type="l",xlim = c(1.85 ,4.4), ylim = c(0, 13),
     xlab = "Estimated Coefficients", ylab = "Density", main ="Density Plot of Estimates")
lines(denBeta2$x,denBeta2$y, lty = 2)
lines(denBeta3$x,denBeta3$y, lty = 3)
legend("topright", legend=c("Population 1", "Population 2", "Population 3"), lty=1:3)
dev.off()

```

```{r, echo=FALSE}
knitr::include_graphics('Plot1.png')
```


Based on our simulation, we estimated 400 random instances of $\hat { \beta } _ {1,j  }$ for each of the three Populations($Y _ { j } , X _ { 1 } , \varepsilon _ { j } ^ { * }$ ). For each population, these estimates follow a continuous distribution. In order to further study these probability density functions, we approximate them using a kernel estimator:

$$
  \hat { f } _ { h } ( x ) = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } K _ { h } \left( x - x _ { i } \right) = \frac { 1 } { n h } \sum _ { i = 1 } ^ { n } K \left( \frac { x - x _ { i } } { h } \right)
$$
  where we choose *K* to be the *Gaussian Kernel*, which is the conventional choice and produces a standard normal density function. That means that $K ( x ) = \phi ( x )$, where $\phi$ is the standard normal density function. Given that we expect our estimates to be approximately normally distributed (see below), this is a natural choice for our kernel density estimation.
The smoothing parameter  *h*  is called the bandwidth and had to be choosen according to the data. The ideal bandwidth is as small as possible to avoid loss of information and as big as necessary to avoid overfitting and thus represent the true density of the estimates. If the bandwith is to small, our function curve will be undersmoothed. It will then overreact to idiosyncrasies in the data and contain jumps that are not in line with the true density function. Too large a bandwidth on the other hand will smoothe the estimated density function too much to the point where it can no longer display meaningful deviations. The most extrem example for this case is a flat line (global mean).
A rule-of-thumb for the bandwidth in R is "Silvermann's *rule of thumb*", which is 0.9 times the minimum of the standard deviation and the interquartile range divided by 1.34 times the sample size to the negative one-fifth power (See man page of function density() in R). It is used as a default for calculating the density in R and produced good results given our data. We also estimated the density plot with a slightly reduced bandwidth, which lead to a unstable distribution picture, especially for population 1. Therefore we remained with the default setting of R.

```{r, echo=FALSE}
## KH:
## - habe die Formulierung in der Interpretation etwas verändert, wo zum kernel density plot
##   hingeführt wird
## - Fortschrittsanzeige für die Schleife
## - overfitting als begriff ergänzt bei der bandwidth diskussion und später "global mean" als Folge
## - spell-checked
## - wie die formel für die bandwidth genau ist, kann man vielleicht rauskürzen, wenn das nötig wird;
##   ich denke die Begründung für die gewählte Variante ist gut
```

> i.) $[ 1.5 \mathrm { P } ]$ Now concentrate on the kernel density plot based on the first population (i.e.: $Y _ { 1 } , X _ { 1 } , \varepsilon _ { 1 } ^ { * }$). Does the distribution of $\hat { \beta } _ { 1 }$ conform to your expectations? More specifically, explain which distribution you would expect (and why) and whether the plotted density conform to that expectation (no formal tests necessary).

```{r, echo=FALSE}
knitr::include_graphics('Plot2.png')
```

Given the basic properties of an ML-estimator we expect our estimates $\hat { \beta } _ {1,j}$ to be realizations of a random variable with a probability density that, for our large sample, approximates normality with very low variance (asymptotic efficiency) and we expect the mean parameter estimate to converge to the true parameter ${ \beta } _ {1,j}$. 
Therefore we expect $\hat { \boldsymbol { \beta } _ {1,j}}$ to follow
$$
  \hat { \boldsymbol { \beta }_ {1} }  \sim \operatorname { Normal } ( \boldsymbol { \beta }_ {1} , \mathbf { V }  )
$$
  with the asymptotic variance of $\hat { \beta } _ {1,j  }$
  $$
  \operatorname { Avar } ( \hat { \boldsymbol { \beta } _ {1,j  } } ) \equiv \left\{  \frac { \left[ \phi \left( \mathbf { x } _ { 1 } \hat { \boldsymbol { \beta }_ {1,j  } } \right) \right] ^ { 2 } \mathbf { x } _ { 1 } ^ { \prime } \mathbf { x } _ { 1 } } { \Phi \left( \mathbf { x } _ { 1 } \hat { \boldsymbol { \beta }_ {1,j  } } \right) \left[ 1 - \Phi \left( \mathbf { x } _ { 1 } \hat { \boldsymbol { \beta }_ {1,j  } } \right) \right] } \right\} ^ { - 1 } \equiv \hat { \mathbf { V } }
$$
  The plotted density for population 1 meets our expectation by first glance. The estimates of  $\hat { \beta } _ {1,j  }$ for our first population concentrate around its expected value of 4, which is equivalent to the true parameter ${ \beta } _ { 1 }$ of our latent model. 
  
```{r, echo=FALSE}
## KH:
## - das hier ist aus Wooldridge, oder? da steht noch eine Summation dieses Bruches über i
##   guck hier bitte nochmal nach, ob das so stimmt und ergänz es ggf. im LaTeX-Teil
## - vielleicht sollten wir hier wooldridges buch zitieren, weil die formel auf unseren folien
##   etwas anders ist (?)
```
  

> ii.) $[ 1 \mathrm { P } ]$ Compare the distributions of $\widehat { \beta _ { 1 } }$ from the population j = 1 to the $\widehat { \beta _ { 1 } }$  from the population j = 2 and j = 3, respectively. Why do the means of the distributions differ?
  
  As we can can see in Plot 1, all 3 distributions are approximatly normally distributed, but differ in their means and variances. For the means we find that ${\hat \beta } _ { 1,1 }$ = `r round(mean(estBeta1),2)`, ${\hat \beta } _ { 1,2 }$ = `r round(mean(estBeta2),2)` and ${\hat \beta } _ { 1,3 }$ = `r round(mean(estBeta3),2)`.
  
  For population one, the error term was independent of the regressor and standard normally distributed. Since we know that ${\hat \beta } _ { 1 }$ is asymptotically normally distributed around the true parameter value, we expect the estimate of ${\hat \beta } _ { 1 }$ to be close to the true value of 4 as is the case. In the second population, the error term has a higher variance. This results in a decreased estimate of ${\hat \beta } _ { 1 }$ (see b iii.). In the third population, the error variance is again set to 2 and also the independence of the regressor and the error term in the latent model is violated. This endogeneity means that the conditional expectation of the error term given x is unequal to 0. Therefore, the estimate of ${\hat \beta } _ { 1 }$ is inconsistent. 
  
```{r, echo=FALSE}
## KH:
## - fand hier die Formulierung etwas unklar, darum habe ich das noch ein kleines 
##   bisschen ausgebaut; schau hier am besten nochmal drüber, aber ich denke, das sollte so passen
## - für population 3 weiß ich nicht, wie man auf den neuen mean kommt. hier sind die gauss-markov-annahmen
##   für die eigenschaften des MLE verletzt, aber ich find nicht raus, ob das einen bias impliziert oder obs
##   an der Inkonsistenz des Schätzers liegt oder ob es was Anderes ist


res <- resid(lm(yLatent3 ~ 1 + distr3$xOne + distr3$error3))

summary(glm(distr3$Y ~ 1 + distr3$error3, family=binomial(link="probit")))

summary(glm(distr3$Y ~ 1 + res, family=binomial(link="probit")))


```
  
  > iii.) $[ 1 \mathrm { P } ]$ Compute the mean of $\widehat { \beta _ { 1 } }$ from the population j = 2. Can you explain, why the distribution of this particular $\widehat { \beta _ { 1 } }$  concentrate approximately around this value?
    
In the second population, the latent variable $y*$ depends not only on the regressor $\mathbf{x}$ but also on a (weighted) second variable $\gamma c \text { which is independent of } \mathbf { x } \text { and } c \sim \text { Normal} \left( 0 , \tau ^ { 2 } \right)$ with $\tau ^ {2} \neq 1$. This constitutes a case of neglected heterogeneity. The probablity of $y* > 0$ given \mathbf{x} and $c$ is then
  $$
    \mathrm { P } ( y = 1 | \mathbf { x } , c ) = \Phi ( \mathbf { x } \beta + \gamma c ).
    $$
       Since $c$ is neglected, it's effect is part of the error term in the regression. This probit function consistenly estimates plim $\boldsymbol {\hat \beta } / \sigma$ where $\sigma^2$ is the variance of the error term. The variance of the error term is larger than 1, however, because it is inflated by the variance of $c$ which is $\gamma ^2\tau^2$. Consequentially, the parameter estimate $\boldsymbol {\hat \beta }$ is attenuated by this variance $\sigma^2$. In our population 2, the error term has variance \sigma2. The parameter estimate $\boldsymbol {\hat \beta }$ is therefore halved from 4 to 2 (attentuation bias).

```{r, echo=FALSE}
## KH:
## - ich hab hier etwas verkürzt und hab probiert, die Erklärung noch näher an die Frage anzupassen
```
  
>  Simulate again all three populations as you did in a). But this time, estimate and save the average marginal probability effect of $x _ { 1 }$ Repeat the estimation 400 times with a sample size of each iteration equaling 30000.

```{r include=FALSE}
## create empty vectors for our estimates
AMPEEst1 <- NULL
AMPEEst2 <- NULL
AMPEEst3 <- NULL

# 
for (i in 1:n){
  ## Population 1
  distr1 <- rmvnorm(obs, mean = c(10,0) ,sigma = V1 , method = 'eigen') %>% as.data.frame()
  names(distr1) = c('xOne','error1')
  distr1$xOne <- round(distr1$xOne,2)
  yLatent1 <- beta0 + beta1 * distr1[,"xOne"] + distr1[,"error1"]
  y1 <- yLatent1 %>% replace(yLatent1<=0,0) %>% replace(yLatent1>0,1)
  distr1 <- distr1 %>% mutate(Y = y1)
  probitAMPE1 <- probitmfx(Y~xOne,data=distr1,atmean=FALSE)
  AMPEEst1[i] <- probitAMPE1$mfxest[1]
  ## Population 2
  distr2 <- rmvnorm(obs, mean = c(10,0) ,sigma = V2 , method = 'eigen') %>% as.data.frame()
  names(distr2) = c('xOne','error2')
  distr2$xOne <- round(distr2$xOne,2)
  yLatent2 <- beta0 + beta1 * distr2[,"xOne"] + distr2[,"error2"]
  y2 <- yLatent2 %>% replace(yLatent2<=0,0) %>% replace(yLatent2>0,1)
  distr2 <- distr2 %>% mutate(Y = y2)
  probitAMPE2 <- probitmfx(Y~xOne,data=distr2,atmean=FALSE)
  AMPEEst2[i] <- probitAMPE2$mfxest[1]
  ## Population 3
  distr3 <- rmvnorm(obs, mean = c(10,0) ,sigma = V3 , method = 'eigen') %>% as.data.frame()
  names(distr3) = c('xOne','error3')
  distr3$xOne <- round(distr3$xOne,2)
  yLatent3 <- beta0 + beta1 * distr3[,"xOne"] + distr3[,"error3"]
  y3 <- yLatent3 %>% replace(yLatent3<=0,0) %>% replace(yLatent3>0,1)
  distr3 <- distr3 %>% mutate(Y = y3)
  probitAMPE3 <- probitmfx(Y~xOne,data=distr3,atmean=FALSE)
  AMPEEst3[i] <- probitAMPE3$mfxest[1]
}


# c) Plot the kernel density estimates ------------------------------------

# adjust broder bandwidth to get a smooth function
denAMPE1 <- density(AMPEEst1, bw = "nrd0", adjust = 2, kernel = "gaussian")
denAMPE2 <- density(AMPEEst2, bw = "nrd0", adjust = 2, kernel = "gaussian")
denAMPE3 <- density(AMPEEst3, bw = "nrd0", adjust = 2, kernel = "gaussian")


# plot(denAMPE1$x,denAMPE1$y,type="l",xlim = c(min(AMPEEst1),max(AMPEEst1)), 
#      xlab = "Estimated AMPE j = 1", ylab = "Density", main ="Population 1")
# abline(v=mean(AMPEEst1))
# plot(denAMPE2$x,denAMPE2$y,type="l",xlim = c(min(AMPEEst2),max(AMPEEst2)), 
#      xlab = "Estimated AMPE j = 2", ylab = "Density", main ="Population 2")
# abline(v=mean(AMPEEst2))
# plot(denAMPE3$x,denAMPE3$y,type="l",xlim = c(min(AMPEEst3),max(AMPEEst3)), 
#      xlab = "Estimated AMPE j = 3", ylab = "Density", main ="Population 3")
# abline(v=mean(AMPEEst3))
# dev.off()
## Make one Plot
png(filename="Plot3.png")

plot(denAMPE1$x,denAMPE1$y,type="l",xlim = c(min(AMPEEst1),max(AMPEEst3)), ylim = c(0, 300),
     xlab = "Estimated Coefficients", ylab = "Density", main ="Density Plot of AMPES")
lines(denAMPE2$x,denAMPE2$y, lty = 2)
lines(denAMPE3$x,denAMPE3$y, lty = 3)
legend("topright", legend=c("Population 1", "Population 2", "Population 3"), lty=1:3)

clip(0,0.120,0,max(denAMPE1$y))
abline(v=mean(AMPEEst1))
clip(0,0.120,0,max(denAMPE2$y))
abline(v=mean(AMPEEst2))
clip(0,0.120,0,max(denAMPE3$y))
abline(v=mean(AMPEEst3))
dev.off()


```


> c.) $[ 1 \mathrm { P } ]$ Analogously to b), plot the kernel density estimates for all three AMPE's. Determine the values around which the sample distributions are concentrated.

```{r, echo=FALSE}
knitr::include_graphics('Plot3.png')
```

Again we can see that the sample distributions (with N = 400) are approximately normal and concentrated around their means. For population j = 1 we get `r mean(AMPEEst1)` for $\hat { E } [ A M P E | j = 1 ]$, for population j = 2 we get `r mean(AMPEEst2)` for $\hat { E } [ A M P E | j = 2 ]$ and for population j = 3 we get `r mean(AMPEEst3)` for $\hat { E } [ A M P E | j = 3 ]$.


> i.) $[ 1.5 \mathrm { P } ]$ Calculate the relative difference (in percent) between $\hat { E } [ A M P E | j = 1 ]$ and $\hat { E } [ A M P E | j = 3 ]$. Which estimate would you use to ascertain the effect of the variable $x _ { 1 }$? 


```{r, echo=FALSE}
relDiff1 <- abs((mean(AMPEEst1) - mean(AMPEEst3)) / mean(AMPEEst1))
```

The relative difference between $\hat { E } [ A M P E | j = 1 ]$ and $\hat { E } [ A M P E | j = 3 ]$ is `r relDiff1*100` percent, which is rather big and therefore we have to be careful, which one of the two we would like to choose for a valid interpretation of the marginal effect of variable $x _ { 1 }$. In this case, the estimate of $\hat { E } [ A M P E | j = 1 ]$ is more reliable since in the model for the population 1 we have a) the normality of the error term and b) its independence of the variable $x _ { 1 }$. Therefore, $\hat { E } [ A M P E | j = 1 ]$ can be estimated consistently as opposed to $\hat { E } [ A M P E | j = 3 ]$ which is estimated from a model of population 3, which violates both assumptions necessary for asymptotic efficiency of the estimation.  

> ii.) $[ 0.5 \mathrm { P } ]$ Calculate the relative difference (in percent) between $\hat { E } [ A M P E | j = 1 ]$ and $\hat { E } [ A M P E | j = 2 ]$. 

```{r, echo=FALSE}
relDiff2 <- abs((mean(AMPEEst1) - mean(AMPEEst2)) / mean(AMPEEst1))
```

The relative difference between $\hat { E } [ A M P E | j = 1 ]$ and $\hat { E } [ A M P E | j = 2 ]$ is `r relDiff2*100` percent.

> iii.) $[ 3 \mathrm { P } ]$  Based on the results in b-ii) would you expect the differences observed in c-i) and  c-ii)? Please provide a detailed explanation for the observed results.

Based on our Plot 1, the results in c-i) and c-ii) are not intuitive, since $\hat { \beta }_1$ for population 1 and 3 are much closer to each other than to $\hat { \beta }_1$ for population 2.

However, since we know that in case of population 2 where we suspect unobserved heterogeneity in the error term and ${x}$ being uncorrelated, the attentuation bias will average out across its distribution during the AMPE estimation. Therefore, in this case the probit model will consistently estimate the average partial effects, even when we account for unobserved heterogenity (due to unnormality of the error term). Since both AMPE estimates of population j = 1 and j = 2 are consistent, we expect their value to be very close, which is confirmed in our Plot 3, and their relative diffrence to be very small (c-ii).

In population 3 we also deal with suspected unobserved heterogeneity, however here the error term is correlated with $x$. In this case it is not possible to estimate $\hat { \beta } _ { 1,3 }$ and the associated $\hat { E } [ A M P E | j = 3 ]$ consistently. The inconsistence of the estimates therefore will explain the relatively big discrepancy between $\hat { E } [ A M P E | j = 1 ]$ and $\hat { E } [ A M P E | j = 3 ]$ observed on the Plot 3 and relative big diffrence calculated in c)i. 


$$
\pagebreak
$$

## Problem 2: (Marginal effects estimation & Interpretation):

Load the dataset “south_african_heart_disease_data.dta” and estimate the effect of ldl-
(bad)cholesterol (ldl) in blood on the probability of suffering from heart disease (chd
equals 1 if one suffers from it).

> a.) $[ 0.5 \mathrm { P } ]$ Can you learn anything from the estimated coefficients? Explain shortly.

```{r, echo=FALSE}


```

> b.) $[ 0.5 \mathrm { P } ]$ Are the S.E. valid, or do you need to adjust them for heteroscedasticity?
Explain.


```{r, echo=FALSE}


```

> c.) $[ 0.5 \mathrm { P } ]$ Re-estimate the model from a) but this time include age in addition to ldl.
You see that the estimated coefficient of ldl changes. Explain why? Additionally,
show that your explanation is supported by the data.

```{r, echo=FALSE}


```

> d.) Finally, estimate the model from a) but include ldl squared next to ldl as a control variable.

```{r, echo=FALSE}


```

> i.) $[ 1 \mathrm { P } ]$ Based on the estimated coefficients from a) and d) draw the two
resulting marginal probability effects of ldl as a function of ldl for ldl $\in$
[1; 15] next to each other.

```{r, echo=FALSE}


```

> ii.) $[ 0.5 \mathrm { P } ]$ Are any of the marginal probability effects linear in ldl? Explain why.

```{r, echo=FALSE}


```

> iii.) $[ 1 \mathrm { P } ]$ What is the advantage of the marginal probability effect based on the
estimation in d) over the one based on a)? Explain shortly.


```{r, echo=FALSE}


```

> iv.) $[ 1.5 \mathrm { P } ]$ Calculate and properly interpret both marginal probability effects for
the mean value of ldl in the sample (You do not need to compute standard
errors).

```{r, echo=FALSE}


```

> iv.) $[ 1 \mathrm { P } ]$ Are any of the effects computed in iv), ceteris paribus effects? Explain
shortly.

