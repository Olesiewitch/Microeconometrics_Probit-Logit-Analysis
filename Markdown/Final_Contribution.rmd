---
title: "First Assignment"
author: "Malgorzata, Kevin, Johannes"
date: "December, 2018"
output: 
  pdf_document:
    latex_engine: xelatex
fontsize: 11pt
---

## First Problem: (Simulation: Latent Variable, Probit Model):
$$
\begin{array} { l } 
{ \text { Consider the following JOINT distributions of random variables } X _ { 0 }  \&  \varepsilon _ { j } ^ { * } \text { (You must simulate } } \\ { \text { it as a joint (multivariate) normal distributions!). } } 
\end{array}
$$
$$
\left( 
\begin{array} { c } 
{ X _ { 0 } } \\ { \varepsilon _ { 1 } ^ { * } } \end{array} \right) \sim N \left[ \left( \begin{array} { c } { 10 } \\ { 0 } \end{array} \right) , \left( \begin{array} { c c } { 2 ^ { 2 } } & { 0 } \\ { 0 } & { 1 } \end{array} \right) \right] , \left( \begin{array} { c } { X _ { 0 } } \\ { \varepsilon _ { 2 } ^ { * } } \end{array} \right) \sim N \left[ \left( \begin{array} { c } { 10 } \\ { 0 } \end{array} \right) , \left( \begin{array} { c c } { 2 ^ { 2 } } & { 0 } \\ { 0 } & { 2 ^ { 2 } } \end{array} \right) \right] , 
$$

$$
\left( 
\begin{array} { c } { X _ { 0 } } \\ { \varepsilon _ { 3 } ^ { * } } \end{array} \right) \sim N \left[ \left( \begin{array} { c } { 10 } \\ { 0 } \end{array} \right) , \left( \begin{array} { c c } { 2 ^ { 2 } } & { 3 } \\ { 3 } & { 2 ^ { 2 } } \end{array} 
\right) \right]
$$

with 

$$
\begin{array} { l } 
{ \beta _ { 0 } = - 30 } \\ { \beta _ { 1 } = 4 } \\
\\
{ Y _ { j } ^ { * } = \beta _ { 0 } + \beta _ { 1 } * X _ { 1 } + \varepsilon _ { j }} \\
\\
Y _ { j } = \left\{ \begin{array} { l l } { 1 \text { if } Y _ { j } ^ { * } > 0 } \\ { 0 , \text { otherwise } } \end{array} \right.\\
\\
j \in \{ 1,2,3 \}
\end{array}
$$

> a) [1P] Please simulate the three datasets (i.e.: $Y _ { j } , X _ { 1 } , 
\varepsilon _ { j } ^ { * } ; j \in \{ 1,2,3 \}$ ) with a sample size of 30000 observations.
For each of the three datasets, estimate a probit model of $\mathrm { Y } _ { \text { i } } \text { on } x _ { 1 }$ 
and save the estimate for $\hat { \beta } _ { 1 }$.

```{r include=FALSE}
library(tidyverse, quietly = TRUE) # for pipe operator %>%
library(mvtnorm, quietly = TRUE) # for joint multivariate Distribution
library(mfx, quietly = TRUE) # for estimating AMPE

setwd("~/Desktop/Master_Statistik/WS-18-19/Courses/Microeconometrics/Tutorial/1assigment")
```


```{r, echo=FALSE}
# Assigment 1 ------------------------------------------------------------------

# clear environment
rm(list = ls())

# set seed for comparibility
set.seed(101)
# ------------------------------------------------------------------------------
## Define Parameters for the latent model
beta0 <- -30
beta1 <- 4
# Number of Observations 30.000 (use 400 for fast results)
obs = 30000

# Simulate given Distributions -------------------------------------------------

# Use package mvnorm for simulating multivariate Data with given vetcor of means
# and given Covariance Matrix V
# method used described by Ripley (1987, p.98)

## Define Covariance Matrix V for Population j = 1,2,3
V1 <- matrix(c(4,0,0,1), nrow=2, ncol=2)
V2 <- matrix(c(4,0,0,4), nrow=2, ncol=2)
V3 <- matrix(c(4,3,3,4), nrow=2, ncol=2)

# 1.a) Simulate three Datasets with Y, xNull and error -------------------

## Simulate distributions with given means and Covariance; round xNull 
distr1 <- rmvnorm(obs, mean = c(10,0) ,sigma = V1 , method = 'eigen') %>% as.data.frame() 
names(distr1) = c('xNull','error1')
distr1$xNull <- round(distr1$xNull,2)
distr2 <- rmvnorm(obs, mean = c(10,0) ,sigma = V2 , method = 'eigen') %>% as.data.frame()
names(distr2) = c('xNull','error2')
distr2$xNull <- round(distr2$xNull,2)
distr3 <- rmvnorm(obs, mean = c(10,0) ,sigma = V3 , method = 'eigen') %>% as.data.frame()
names(distr3) = c('xNull','error3')
distr3$xNull <- round(distr3$xNull,2)

# Latent Model for the joint multivariate Distributions
yLatent1 <- beta0 + beta1 * distr1[,"xNull"] + distr1[,"error1"]
yLatent2 <- beta0 + beta1 * distr2[,"xNull"] + distr2[,"error2"]
yLatent3 <- beta0 + beta1 * distr3[,"xNull"] + distr3[,"error3"]
## transform Latent Variable into Bernoulli Variable
y1 <- yLatent1 %>% replace(yLatent1<=0,0) %>% replace(yLatent1>0,1)
y2 <- yLatent2 %>% replace(yLatent2<=0,0) %>% replace(yLatent2>0,1)
y3 <- yLatent3 %>% replace(yLatent3<=0,0) %>% replace(yLatent3>0,1)
distr1 <- distr1 %>% mutate(Y = y1)
distr2 <- distr2 %>% mutate(Y = y2)
distr3 <- distr3 %>% mutate(Y = y3)
```

```{r include=FALSE}
# 1.a) Save the estimate for Betaj, j = 1,2,3 -----------------------------

# Probit Model
probit1<-glm(distr1[,"Y"] ~ distr1[,"xNull"], family=binomial(link="probit")) 
probit2<-glm(distr2[,"Y"] ~ distr2[,"xNull"], family=binomial(link="probit")) 
probit3<-glm(distr3[,"Y"] ~ distr3[,"xNull"], family=binomial(link="probit")) 
```

```{r, echo=FALSE}
estBeta1 <- round(probit1$coefficients[2], 2)
estBeta2 <- round(probit2$coefficients[2], 2)
estBeta3 <- round(probit3$coefficients[2], 2)
```

Given our simulated Data, we calculate the Probit model for our binary dependend variable $y _ { i }$ given $x _ { 1 }$  
$$
f \left( y _ { i } | \mathbf { x } _ { 1 } \right) = \left[ \Phi \left( \mathbf { x } _ { 1 } ^ { \prime } \mathbf { \beta _ {1} } \right) \right] ^ { y _ { i } } \left[ 1 - \Phi \left( \mathbf { x } _ { 1 } ^ { \prime } \boldsymbol { \beta } _ {1} \right) \right] ^ { 1 - y _ { i } } \quad y _ { i } = 0,1
$$
to get the probit estimator $\hat { \beta } _ {1,j  }$ for j = 1,2,3 through the maximization of the log likelihood of our density function $f \left( y _ { i } | \mathbf { x } _ { 1 } \right)$.

The estimates for $\hat { \beta } _ { 1,1 } , \hat { \beta } _ { 1,2 } \text{ and } \hat { \beta } _ { 1,3 }$ are `r estBeta1`, `r estBeta2` and `r estBeta3` respectively.

> b) [1.5P] Repeat a 400 times while saving all the different estimate in vectors for all
three models (based on the three different populations). Plot the kernel density
estimates for $\hat { \beta } _ { 1 }$ based on the three populations next to each other. Describe shortly
the choices you faced and made estimating the density functions.


```{r, echo=FALSE}
# b.) Repeat estimation 400 times -----------------------------------------
## Number of repeated iterations n
n <- 400
# empty vectors for estimated betas for each population
betaEst1 <- NULL
betaEst2 <- NULL
betaEst3 <- NULL
```

```{r include=FALSE}
for (i in 1:n){
  ## Population 1
  distr1 <- rmvnorm(obs, mean = c(10,0) ,sigma = V1 , method = 'eigen') %>% as.data.frame()
  names(distr1) = c('xOne','error1')
  distr1$xOne <- round(distr1$xOne,2)
  yLatent1 <- beta0 + beta1 * distr1[,"xOne"] + distr1[,"error1"]
  y1 <- yLatent1 %>% replace(yLatent1<=0,0) %>% replace(yLatent1>0,1)
  distr1 <- distr1 %>% mutate(Y = y1)
  probit1<-glm(distr1[,"Y"] ~ distr1[,"xOne"], family=binomial(link="probit"))
  betaEst1[i] <- probit1$coefficients[2]
  ## Population 2
  distr2 <- rmvnorm(obs, mean = c(10,0) ,sigma = V2 , method = 'eigen') %>% as.data.frame()
  names(distr2) = c('xOne','error2')
  distr2$xOne <- round(distr2$xOne,2)
  yLatent2 <- beta0 + beta1 * distr2[,"xOne"] + distr2[,"error2"]
  y2 <- yLatent2 %>% replace(yLatent2<=0,0) %>% replace(yLatent2>0,1)
  distr2 <- distr2 %>% mutate(Y = y2)
  probit2<-glm(distr2[,"Y"] ~ distr2[,"xOne"], family=binomial(link="probit"))
  betaEst2[i] <- probit2$coefficients[2]
  ## Population 3
  distr3 <- rmvnorm(obs, mean = c(10,0) ,sigma = V3 , method = 'eigen') %>% as.data.frame()
  names(distr3) = c('xOne','error3')
  distr3$xOne <- round(distr3$xOne,2)
  yLatent3 <- beta0 + beta1 * distr3[,"xOne"] + distr3[,"error3"]
  y3 <- yLatent3 %>% replace(yLatent3<=0,0) %>% replace(yLatent3>0,1)
  distr3 <- distr3 %>% mutate(Y = y3)
  probit3<-glm(distr3[,"Y"] ~ distr3[,"xOne"], family=binomial(link="probit"))
  betaEst3[i] <- probit3$coefficients[2]
}

## Make Plots for the answers
# Plot the kernel density estimates for beta based on the three populations ----
denBeta1 <- density(betaEst1, bw = "nrd0", adjust = 1, kernel = "gaussian")
denBeta2 <- density(betaEst2, bw = "nrd0", adjust = 1, kernel = "gaussian")
denBeta3 <- density(betaEst3, bw = "nrd0", adjust = 1, kernel = "gaussian")

png(filename="Plot2.png", height = 370)

plot(denBeta1$x,denBeta1$y,type="l",xlim = c(min(betaEst1) ,max(betaEst1)),
     xlab = "Estimated Coefficient", ylab = "Density", main ="Population 1")
abline(v=mean(betaEst1))
dev.off()
# 
# 
png(filename="Plot1.png")

plot(denBeta1$x,denBeta1$y,type="l",xlim = c(1.85 ,4.4), ylim = c(0, 13),
     xlab = "Estimated Coefficients", ylab = "Density", main ="Density Plot of Estimates")
lines(denBeta2$x,denBeta2$y, lty = 2)
lines(denBeta3$x,denBeta3$y, lty = 3)
legend("topright", legend=c("Population 1", "Population 2", "Population 3"), lty=1:3)
dev.off()

```

```{r, echo=FALSE}
knitr::include_graphics('Plot1.png')
```


Based on our simulation, we estimated 400 random instances of $\hat { \beta } _ {1,j  }$ for each of the three Populations($Y _ { j } , X _ { 1 } , \varepsilon _ { j } ^ { * }$ ). In order to be able to interprete our distributions, we need to find a smooth representation for the probability density function of our estimates. The typical non-parametric way to achieve a smooth function is to make use of a kernel estimator 

$$
\hat { f } _ { h } ( x ) = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } K _ { h } \left( x - x _ { i } \right) = \frac { 1 } { n h } \sum _ { i = 1 } ^ { n } K \left( \frac { x - x _ { i } } { h } \right)
$$
where we choose *K* to be a *Gaussian Kernel*, which is the convential choice and produces a standard normal density function. That means that $K ( x ) = \phi ( x )$, where $\phi$ is the standard normal density function. Given that we expect our estimate to be approximately normal (see below), this is the best choice for our kernel density estimation.
The smoothing parameter  *h*  is called the bandwidth and had to be choosen accordingly to the data. The goal is to find a bandwidth that is as small as possible to avoid loss of information and as big as necessary to represent the true density of our estimates. If the bandwith is to small, our function curve will be undersmoothed. which means that our function will contain jumps that are not part of our true density. On the other side, if our bandwidth is to big and our curve will be oversmoothed. The most extrem example for this case is a flat line.
A rule-of-thumb for the bandwidth in R is "Silvermann's *rule of thumb*", which is 0.9 times the minimum of the standard deviation and the interquartile range divided by 1.34 times the sample size to the negative one-fifth power (See man page of function density() in R). It is used as a default for calculating the density in R and produced good result given our data. We also estimated the density plot with a slightly reduced bandwidth, which lead to a unstable distribution picture, especially for population 1. Therefore we remained with the default setting of R.

> i.) $[ 1.5 \mathrm { P } ]$ Now concentrate on the kernel density plot based on the first population (i.e.: $Y _ { 1 } , X _ { 1 } , \varepsilon _ { 1 } ^ { * }$). Does the distribution of $\hat { \beta } _ { 1 }$ conform to your expectations? More specifically, explain which distribution you would expect (and why) and whether the plotted density conform to that expectation (no formal tests necessary).

```{r, echo=FALSE}
knitr::include_graphics('Plot2.png')
```

Given the basic properties of a ML-estimator we expect our estimates of $\hat { \beta } _ {1,j}$ to be a random variable with probability distribution that is approximately normal and to converge in probability to the true parameter ${ \beta } _ {1,j}$. 
Therefore we expect $\hat { \boldsymbol { \beta } _ {1,j}}$ to follow
$$
\hat { \boldsymbol { \beta }_ {1} }  \sim \operatorname { Normal } ( \boldsymbol { \beta }_ {1} , \mathbf { V }  )
$$
with the asymptotic variance of $\hat { \beta } _ {1,j  }$
$$
\operatorname { Avar } ( \hat { \boldsymbol { \beta } _ {1,j  } } ) \equiv \left\{  \frac { \left[ \phi \left( \mathbf { x } _ { 1 } \hat { \boldsymbol { \beta }_ {1,j  } } \right) \right] ^ { 2 } \mathbf { x } _ { 1 } ^ { \prime } \mathbf { x } _ { 1 } } { \Phi \left( \mathbf { x } _ { 1 } \hat { \boldsymbol { \beta }_ {1,j  } } \right) \left[ 1 - \Phi \left( \mathbf { x } _ { 1 } \hat { \boldsymbol { \beta }_ {1,j  } } \right) \right] } \right\} ^ { - 1 } \equiv \hat { \mathbf { V } }
$$
The plotted density confirms our expectation by first glance. The estimates of  $\hat { \beta } _ {1,j  }$ for our first population concentrate around its expected value of 4, which is equivalent to the true parameter ${ \beta } _ { 1 }$ of our latent model. 

> ii.) $[ 1 \mathrm { P } ]$ Compare the distributions of $\widehat { \beta _ { 1 } }$ from the population j = 1 to the $\widehat { \beta _ { 1 } }$  from the population j = 2 and j = 3, respectively. Why do the means of the distributions differ?

As we can can see in Plot 1 all 3 distributions are approximatly normally distributed, but differ in their means and variances. For the means we find that ${\hat \beta } _ { 1,1 }$ = `r round(mean(estBeta1),2)`, ${\hat \beta } _ { 1,2 }$ = `r round(mean(estBeta2),2)` and ${\hat \beta } _ { 1,3 }$ = `r round(mean(estBeta3),2)`.
As we explained earlier the simulated errors of ${\hat \beta } _ { 1 }$ follow a standard normal distribution and we expect the estimate of ${\hat \beta } _ { 1 }$ to be close to the true value of 4. In the second population our errors are not standard normal and intruduce a attenuation bias.
The third population introduces a Covariance between the explanatory variable and the errorterm in our latent model. Therefore our error term is not independent of the explanatory variable in our latent model and the estimate of ${\hat \beta } _ { 1 }$ from the population j = 3 is inconsistent. 

> ii.) $[ 1 \mathrm { P } ]$ Compute the mean of $\widehat { \beta _ { 1 } }$ from the population j = 2. Can you explain, why the distribution of this particular $\widehat { \beta _ { 1 } }$  concentrate approximately around this value?

We know from the issue of Neglected Heterogenity, which has the structural model of  
$$
\mathrm { P } ( y = 1 | \mathbf { x } , c ) = \Phi ( \mathbf { x } \beta + \gamma c ),
$$
where $\gamma c$ represents omitted variables which are independent of x and typically included in the errorterm. We assume that $c \text { is independent of } \mathbf { x } \text { and } c \sim \text { Normal } \left( 0 , \tau ^ { 2 } \right)$.
Now we know that probit of y on x consistently estimates plim $\boldsymbol {\hat \beta } / \sigma$. We call this the attentuation bias.
Our error term is 
$$
\epsilon = u + \gamma c \sim N \left( 0 , \gamma ^ { 2 } \tau ^ { 2 } + 1 \right)
$$
and we know from our distribution that $\epsilon\sim \text { Normal } \left( 0 , 4 \right)$. Therefore we know that $\sigma = 4^{1/2}$. Our estimated mean of ${\hat \beta } _ { 1,2 }$ is approximately equal to the value of ${\beta } / \sigma$ = 2.



>  Simulate again all three populations as you did in a). But this time, estimate and save the average marginal probability effect of $x _ { 1 }$ Repeat the estimation 400 times with a sample size of each iteration equaling 30000.

```{r include=FALSE}
## create empty vectors for our estimates
AMPEEst1 <- NULL
AMPEEst2 <- NULL
AMPEEst3 <- NULL

# 
for (i in 1:n){
  ## Population 1
  distr1 <- rmvnorm(obs, mean = c(10,0) ,sigma = V1 , method = 'eigen') %>% as.data.frame()
  names(distr1) = c('xOne','error1')
  distr1$xOne <- round(distr1$xOne,2)
  yLatent1 <- beta0 + beta1 * distr1[,"xOne"] + distr1[,"error1"]
  y1 <- yLatent1 %>% replace(yLatent1<=0,0) %>% replace(yLatent1>0,1)
  distr1 <- distr1 %>% mutate(Y = y1)
  probitAMPE1 <- probitmfx(Y~xOne,data=distr1,atmean=FALSE)
  AMPEEst1[i] <- probitAMPE1$mfxest[1]
  ## Population 2
  distr2 <- rmvnorm(obs, mean = c(10,0) ,sigma = V2 , method = 'eigen') %>% as.data.frame()
  names(distr2) = c('xOne','error2')
  distr2$xOne <- round(distr2$xOne,2)
  yLatent2 <- beta0 + beta1 * distr2[,"xOne"] + distr2[,"error2"]
  y2 <- yLatent2 %>% replace(yLatent2<=0,0) %>% replace(yLatent2>0,1)
  distr2 <- distr2 %>% mutate(Y = y2)
  probitAMPE2 <- probitmfx(Y~xOne,data=distr2,atmean=FALSE)
  AMPEEst2[i] <- probitAMPE2$mfxest[1]
  ## Population 3
  distr3 <- rmvnorm(obs, mean = c(10,0) ,sigma = V3 , method = 'eigen') %>% as.data.frame()
  names(distr3) = c('xOne','error3')
  distr3$xOne <- round(distr3$xOne,2)
  yLatent3 <- beta0 + beta1 * distr3[,"xOne"] + distr3[,"error3"]
  y3 <- yLatent3 %>% replace(yLatent3<=0,0) %>% replace(yLatent3>0,1)
  distr3 <- distr3 %>% mutate(Y = y3)
  probitAMPE3 <- probitmfx(Y~xOne,data=distr3,atmean=FALSE)
  AMPEEst3[i] <- probitAMPE3$mfxest[1]
}


# c) Plot the kernel density estimates ------------------------------------

# adjust broder bandwidth to get a smooth function
denAMPE1 <- density(AMPEEst1, bw = "nrd0", adjust = 2, kernel = "gaussian")
denAMPE2 <- density(AMPEEst2, bw = "nrd0", adjust = 2, kernel = "gaussian")
denAMPE3 <- density(AMPEEst3, bw = "nrd0", adjust = 2, kernel = "gaussian")


# plot(denAMPE1$x,denAMPE1$y,type="l",xlim = c(min(AMPEEst1),max(AMPEEst1)), 
#      xlab = "Estimated AMPE j = 1", ylab = "Density", main ="Population 1")
# abline(v=mean(AMPEEst1))
# plot(denAMPE2$x,denAMPE2$y,type="l",xlim = c(min(AMPEEst2),max(AMPEEst2)), 
#      xlab = "Estimated AMPE j = 2", ylab = "Density", main ="Population 2")
# abline(v=mean(AMPEEst2))
# plot(denAMPE3$x,denAMPE3$y,type="l",xlim = c(min(AMPEEst3),max(AMPEEst3)), 
#      xlab = "Estimated AMPE j = 3", ylab = "Density", main ="Population 3")
# abline(v=mean(AMPEEst3))
# dev.off()
## Make one Plot
png(filename="Plot3.png")

plot(denAMPE1$x,denAMPE1$y,type="l",xlim = c(min(AMPEEst1),max(AMPEEst3)), ylim = c(0, 300),
     xlab = "Estimated Coefficients", ylab = "Density", main ="Density Plot of AMPES")
lines(denAMPE2$x,denAMPE2$y, lty = 2)
lines(denAMPE3$x,denAMPE3$y, lty = 3)
legend("topright", legend=c("Population 1", "Population 2", "Population 3"), lty=1:3)

clip(0,0.120,0,max(denAMPE1$y))
abline(v=mean(AMPEEst1))
clip(0,0.120,0,max(denAMPE2$y))
abline(v=mean(AMPEEst2))
clip(0,0.120,0,max(denAMPE3$y))
abline(v=mean(AMPEEst3))
dev.off()


```


> c.) $[ 1 \mathrm { P } ]$ Analogously to b), plot the kernel density estimates for all three AMPE's. Determine the values around which the sample distributions are concentrated.

```{r, echo=FALSE}
knitr::include_graphics('Plot3.png')
```

Again we can see that the sample distributions (with N = 400) are approximately normal and concentrated their means. For population j = 1 we get `r mean(AMPEEst1)` for $\hat { E } [ A M P E | j = 1 ]$, for population j = 2 we get `r mean(AMPEEst2)` for $\hat { E } [ A M P E | j = 2 ]$ and for population j = 3 we get `r mean(AMPEEst3)` for $\hat { E } [ A M P E | j = 3 ]$.


> i.) $[ 1.5 \mathrm { P } ]$ Calculate the relative difference (in percent) between $\hat { E } [ A M P E | j = 1 ]$ and $\hat { E } [ A M P E | j = 3 ]$. Which estimate would you use to ascertain the effect of the variable $x _ { 1 }$? 


```{r, echo=FALSE}
relDiff1 <- abs((mean(AMPEEst1) - mean(AMPEEst3)) / mean(AMPEEst1))
```

The relative difference between $\hat { E } [ A M P E | j = 1 ]$ and $\hat { E } [ A M P E | j = 3 ]$ is `r relDiff1*100` percent. The relative difference of both estimates is rather big and we have to be careful, which one of the two we would like to choose for a valid interpretation of the effect of variable $x _ { 1 }$. In our case would definately choose the estimate of $\hat { E } [ A M P E | j = 1 ]$ because we know from the distribution parameters of population 1 that our error terms are following a standard normal distribution and therefore that our estimate is consistent and unbiased. We also know, that in Population 3 the errorterms are correlated with our independent variable $x _ { 1 }$ and we assume that the resulting estimate for $\hat { E } [ A M P E | j = 3 ]$ is biased.

> ii.) $[ 0.5 \mathrm { P } ]$ Calculate the relative difference (in percent) between $\hat { E } [ A M P E | j = 1 ]$ and $\hat { E } [ A M P E | j = 2 ]$. 

```{r, echo=FALSE}
relDiff2 <- abs((mean(AMPEEst1) - mean(AMPEEst2)) / mean(AMPEEst1))
```

The relative difference between $\hat { E } [ A M P E | j = 1 ]$ and $\hat { E } [ A M P E | j = 2 ]$ is `r relDiff2*100` percent.

> iii.) $[ 3 \mathrm { P } ]$  Based on the results in b-ii) would you expect the differences observed in c-i) and  c-ii)? Please provide a detailed explanation for the observed results.

Based on our Plot 1, we can expect the differences in c-i) and c-ii). First of all, we know from the definition of average marginal probability effects that the attentuation bias or unobserved heterogenity average out. Therefore the probit of y on x consistently estimates the average partial effects even when we account for unobserved heterogenity. As a result we expect our estimates of poulation j = 1 and j = 2 to be very close, which is conformed in our Plot 3. We also know that we have to compare the magnitude of our estimates from the samples of population 3 with those from population 2, given the fact they have the same variance of error terms and the only difference in population parameters is the correlation of error terms with  $x _ { 1 }$ in poulation 3. Again, as we have seen in Plot 1, the magnitude for our estimate $\hat { E } [ A M P E | j = 3 ]$ is bigger than the estimate of $\hat { E } [ A M P E | j = 2 ]$, which follows the relation between $\hat { \beta } _ { 1,2 }$ and $\hat { \beta } _ { 1,3 }$ from Plot 1. This makes sense, since the average marginal probability effect $\beta _ { K } \mathrm { E } [ \phi ( \mathbf { x } \beta ) ]$ summarizes the estimated marginal effects of $x _ { 1 }$ on $p ( \mathbf { x } )$, which depend on x through $(\phi \mathbf { x } \boldsymbol { \beta } )$.

Therefore the size of $\hat { \beta } _ { 1 }$ is reflected in the average of the estimates for the marginal probability effects. Since $\hat { \beta } _ { 3 }$ is bigger than $\hat { \beta } _ { 2 }$ it makes sense that $\hat { E } [ A M P E | j = 3 ]$ is also bigger than $\hat { E } [ A M P E | j = 2 ]$.


$$
\pagebreak
$$

## Problem 2: (Marginal effects estimation & Interpretation):

Load the dataset “south_african_heart_disease_data.dta” and estimate the effect of ldl-
(bad)cholesterol (ldl) in blood on the probability of suffering from heart disease (chd
equals 1 if one suffers from it).

> a.) $[ 0.5 \mathrm { P } ]$ Can you learn anything from the estimated coefficients? Explain shortly.

```{r, echo=FALSE}


```

> b.) $[ 0.5 \mathrm { P } ]$ Are the S.E. valid, or do you need to adjust them for heteroscedasticity?
Explain.


```{r, echo=FALSE}


```

> c.) $[ 0.5 \mathrm { P } ]$ Re-estimate the model from a) but this time include age in addition to ldl.
You see that the estimated coefficient of ldl changes. Explain why? Additionally,
show that your explanation is supported by the data.

```{r, echo=FALSE}


```

> d.) Finally, estimate the model from a) but include ldl squared next to ldl as a control variable.

```{r, echo=FALSE}


```

> i.) $[ 1 \mathrm { P } ]$ Based on the estimated coefficients from a) and d) draw the two
resulting marginal probability effects of ldl as a function of ldl for ldl $\in$
[1; 15] next to each other.

```{r, echo=FALSE}


```

> ii.) $[ 0.5 \mathrm { P } ]$ Are any of the marginal probability effects linear in ldl? Explain why.

```{r, echo=FALSE}


```

> iii.) $[ 1 \mathrm { P } ]$ What is the advantage of the marginal probability effect based on the
estimation in d) over the one based on a)? Explain shortly.


```{r, echo=FALSE}


```

> iv.) $[ 1.5 \mathrm { P } ]$ Calculate and properly interpret both marginal probability effects for
the mean value of ldl in the sample (You do not need to compute standard
errors).

```{r, echo=FALSE}


```

> iv.) $[ 1 \mathrm { P } ]$ Are any of the effects computed in iv), ceteris paribus effects? Explain
shortly.

