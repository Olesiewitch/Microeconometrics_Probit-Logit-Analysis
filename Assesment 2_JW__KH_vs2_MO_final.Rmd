---
title: "Assesment 2"
author: "Malgorzata Olesiewicz"
date: "31 January 2019"
output: pdf_document
---
---
title: "Microeconometrics 2018/2019 - 1st Assignment"
date: "20th of December, 2018"
output: 
  pdf_document:
    latex_engine: xelatex
fontsize: 11pt
---

#### Instructor: Tomas Repasky
#### Group Members: 
- Johannes Wagner  
Humboldt-Universitat Berlin - ID 598797 - Msc Statistics  
<wagnejoh@hu-berlin.de>
- Malgorzata Paulina Olesiewicz  
Humboldt-Universitat Berlin - ID 598939 - Msc Statistics  
<malgorzata.paulina.olesiewicz@student.hu-berlin.de>

- Kevin Hoppe  
Humboldt-Universitat Berlin - ID 598247 -  Msc Statistics  
<Kevin.Hoppe1@web.de>  

## Approximate individual contributions: 
```{r, echo = FALSE}
suppressWarnings(library("kableExtra"))

a=c("1",
    "2",
    "3",
    "4") 
   

b=c("33 % theory",
    "100% theory",
    "-",
    "??") 

c= c("33 % theory", 
     "-",
     "100% theory + progr",
     "??") 
 
d= c("33% theory", 
     "-",
     "-",
      "100% progr + ??")

work = data.frame(a,b,c,d)
colnames(work)= c("Task","Johannes","Malgorzata","Kevin")
kable(work)%>% kable_styling() 
```


$$
\pagebreak
$$


```{r setup, include=FALSE}
### Setting up the enviroment
library(foreign, quietly = TRUE)
library(tidyverse, quietly = TRUE)
library(margins, quietly = TRUE)
library(mfx, quietly = TRUE)
library(caret,quietly = TRUE)
# install.packages("InformationValue")
library(InformationValue)
library(olsrr)
# install.packages("nnet")
library(nnet)
library(dplyr)
# install.packages("mlogit")
library(mlogit)


```

```{r, echo=FALSE}
# Define Directory
dir <- "~/R/git/Micro_Ass1"

setwd(dir)
# clear environment
rm(list = ls())
### loading the data and looking up meaning of the variables

data <- read.dta("loanapp1.dta")
(variables=cbind(colnames(data),attr(data, "var.labels")))
names(data)
head(data)

# Model Selection ---------------------------------------------------------
# For simplicity reasons we can use a simple OLS model for the selection process
# remove variables which are directly related to rejection and have no social explanatory power: unver
# remove missings for the purpose of model selection
data_woNA <- data[complete.cases(data),] %>% dplyr::select(-c(approve, action, unver))
# names(data_woNA)
# regress the depend variable on all regressors
fit <- lm(reject~.,data=data_woNA)

## Perform forward stepwise selection to find most important variables
ols_step_forward_p(fit, details = FALSE, penter = 0.001)

model=glm(data$reject~sch + married + inson + cons + pubrec + white + obrat, data = dataSelect, family = "binomial"(link="logit"))
## print table
```

## Task 1 : Chosing the variables for the model

The choice of variables should be grounded on both theory and empirical evidence. Our first step was to look at a simple linear regression model for all variables of the data set and did run a forward stepwise selection process on them. We choose a very restrictive significance level of 0.1 % (p-value = 0.001) for our regressors to throw out all regressors with a lower significance level. We obtained nine variables, which could be ordered with regard to their explanatory power (how much they contribute to the Adjusted R square). We then choose five variables from the list of nine with regard to the power of their theoretical contribution to answering the research question and their empirical explanatory power for our sample. Furthermore, for mainly theoretical reasons, we added two variables to control for relevant social characteristics (sch and married). Our theoretocal perspective is guided by the assumption that higher social, cultural and economical capital leads to a lower probability of receiving a loan rejection.

To create the logit model we have chosen followig variables: 

1. Education of 12 or more years (sch): [0,1]; expected 
2. Marital status (married): [0,1]
3. Existence of private mortage insurance (inson): [0,1]
4. Credit History (cons): [1,2,3,4,5,6] 
5. Former bankruptcy (pubrec): [0,1]
6. Ethnic majority (white): [0,1]
7. Ratio of income and liabilities (obrat) : [0-95]

```{r, , echo=FALSE}
## Calculate logit model with MPEs at means
dataSelect <- dplyr::select(data, c(reject, sch, married, inson, cons, pubrec, white, obrat))
logit_results <- logitmfx(reject~sch + married + inson + cons + pubrec + white + obrat, data = dataSelect, atmean=TRUE)
logit_results
```


###Results interpretation
(In general you can not interpret the coefficients of a logit model because they are transformed through g().) 
You can not compare the coefficients because the characteristics of each variable (the mean and the scale) are not the same and the coefficient of the Logit is not a linear parameter.


###Sensitivity and Specificity

The notion of sensitivity and specificity is used to describe how accurately model predicts the binary outcomes. Sensitivity describes the fraction of correctly predicted positive (Y=1) outcomes and specificity describes the fraction of correctly predicted negative (Y=0) outcomes. Our aim is to find an optimal equilibrium between the two fractions, which would allow us simultaneously the best possible prediction of both outcomes.    

In our prediction, we use "c" as a threshold above which the outcome should be predicted as a positive. Consequently, any result of our predicted logit model which will be equal or below the threshold will be assigned a negative outcome. 

The most logical starting point for the binary response prediction model in c =0.5 

```{r}
table(data$reject)
predicted=predict(model,data, type="response")
sen_1=sensitivity(data$reject,predicted,threshold = 0.5)
spec_1=specificity(data$reject,predicted,threshold = 0.5)
print(sen_1) 
print(spec_1)
```

With threshold c=0.5 we can observe very high fraction of negative responds being correctly predicted (98%) but only 29% positive responds have been predicted correctly.Since the closer the threshold to 1 the higher specificity (the likelihood that we will predict negative respond increases) we will decrease the threshold to 0.3. 
```{r}
sen_2=sensitivity(data$reject,predicted,threshold = 0.3)
spec_2=specificity(data$reject,predicted,threshold = 0.3)
print(sen_2) 
print(spec_2)
```
We can observe some improvment in prediction of positive outcomes to 47% and prediction ofnegative respond has still been very accurate - 95%. We are going in the right direction.To find out optimal cut off threshold for the prediction we have used the "InformationValue" package.

```{r}
data_2=data.frame(data$reject, predicted)
data_noNA=data_2[complete.cases(data_2), ]###getting rid off N/A in prediction 

a= optimalCutoff(actuals = data_noNA$data.reject, predictedScores =data_noNA$predicted,optimiseFor="Both",  returnDiagnostics=TRUE)

sen_3=sensitivity(data$reject,predicted,threshold = a$optimalCutoff)
spec_3=specificity(data$reject,predicted,threshold =a$optimalCutoff)

print(a$optimalCutoff)
print(sen_3)
print(spec_3)
```

The optimal cut off level is c=0.088 where 76% of positive respons is being predicted correctly and 72% of negative respons is being predicted correctly. 

### Multinomial Logit


---
title: "Microeconometrics Assignment 2 - Task 4"
author: "KH"
date: "3 Februar 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

setwd("C:/Users/Nutzer/Desktop")

## Packages:

require(foreign)
require(margins)
require(mfx)
require(dplyr)
require(tidyverse)
require(nnet)
require(mlogit)

```



```{r, include=FALSE}

## Load data and extract the 7 variables that we chose 
## as a group:
##
## 1. Education (sch): [0,1]
## 2. marital status (married): [0,1]
## 3. Existence of private mortgage insurance (inson): [0,1]
## 4. Net worth (netw): [-7919, 28023]
## 5. Former bankruptcy (pubrec): [0,1]
## 6. Ethnic majority (white): [0,1]
## 7. Ratio of income and liabilities (obrat) : [0-95]

dat.raw <- read.dta("loanapp1.dta")
dat.red <- select(dat.raw, reject, sch, married, inson, cons, pubrec, white, obrat, netw); as.tibble(dat.red)

## Coding categorial variables as dummy variables:

dummy.var <- function(var){
  
  if(min(unique(var), na.rm = T) == 0){
    var <- var + 1
  }
  
  len.var <- length(var)
  levels.var <- var %>% as.factor %>% levels %>% length
  levels.dummy <- levels.var - 1
  
  dummy <- matrix(NA,
                  nrow = len.var,
                  ncol = levels.dummy)

  for(j in 1:levels.var){
    
    match.loc <- which(var == unique(var)[j])
    
    for(i in match.loc){
      
      dummy.vec <- rep(0, levels.dummy)
      dummy.vec[unique(var)[j] - 1] <- 1
      dummy[i,] <- dummy.vec
    }
  }
  return(dummy)
}

sch <- dummy.var(dat.red$sch); head(sch)
married <- dummy.var(dat.red$married); head(married)
inson <- dummy.var(dat.red$inson); head(inson)
#cons <- dummy.var(dat.red$cons); head(cons)
pubrec <- dummy.var(dat.red$pubrec); head(pubrec)
white <- dummy.var(dat.red$white); head(white)

## Put together data set for regression models:

dat <- data.frame("reject" = dat.red$reject,
                  "educ" = sch,
                  "marry" = married,
                  "insur" = inson,
                  "netw" = dat.red$netw,
                  "bankr" = pubrec,
                  "white" = white,
                  "oblig" = dat.red$obrat)

dat.mnl <- mlogit.data(dat, shape = "wide", choice = "reject")

## Formulas for regression models:

form <- 'reject ~ educ + marry + insur + netw + bankr + white + oblig'

form.mnl <- mFormula(appr ~ educ + marry + insur + netw + bankr + white + oblig)

## Estimate a binomial logit model:

head(dat); binom <- glm(formula = form, data = dat, family = "binomial"(link = "logit"))
summary(binom)

## Estimate a multinomial logit model:

head(dat.mnl); multinom <- multinom(formula = form, data = dat)
summary(multinom)

## Comparison of coefficient estimates:

beta.diff <- cbind(coef(binom) - coef(multinom)); beta.diff

```


The difference in the coefficient estimates is zero up to at least the fourth 
decimal place:

```{r, echo = FALSE, include = TRUE}

beta.diff


```


This is because the multinomial logit model (MNL) reduces to the 
binomial logit model in case of a binomial dependent variable as can be seen in the formulas. In the MNL, the probability $\pi_{ij}$ of individual $i$ choosing alternative $j$ is given by:

  $$ \pi_{ij_{multinomial}} = \frac{\exp(x_{i}'\beta_{j})}{\sum_{r=1}^J \exp{x_{i}'\beta_{j}}}. $$

Compare this to the binomial logit model, where the probability $\pi_{i}$ of individual $i$ 
picking alternative $j = 1$ is given by:

  $$ \pi_{i_{binomial}} = \frac{\exp(x_{i}'\beta)}{1 + \exp(x_{i}'\beta)}.  $$
  
In the MNL, due to identification constraints, $\beta_{1}$ is fixed at $0$. This establishes $j = 1$ as the reference category. For the remaining $J-1$ categories, $\beta_{j}$ coefficients are estimated. If the dependent variable has only two categories, i.e. $J = 2$, this means that only one $\beta$ and one $\pi_{i}$ need to be calculated (for the one category that is not the reference category) so the index $j$ in $\pi_{ij}$ and $\beta_{j}$ can be dropped. Since the constraint for $\beta_{1} = 0$ means that $\exp{(x_{i}'\beta_{1})}$ evaluates to $1$, this reduces the denominator in the MNL to $1 + \exp(x_{i}'\beta_{2})$. After dropping the now obsolete index of the coefficient vector, the two formulas given above are equal.



